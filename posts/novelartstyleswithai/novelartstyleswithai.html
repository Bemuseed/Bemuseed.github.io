<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Creating Novel Art Styles With AI | Caleb Mullan's Site</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The goal of this project is to enable the creation of new, appealing art-styles with AI. The method used to attempt this is training Stable Diffusion LoRa models on specific art styles, then iteratively merging these into new models and comparing their differences mathematically. The training is achieved by preparing a standard set of images, then using ControlNet to &lsquo;convert&rsquo; them into an artist&rsquo;s style.
The project currently consists of a method of training styled LoRa&rsquo;s, and an interactive program that combines and analyses them."><meta name=generator content="Hugo 0.111.3"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=../../ananke/css/main.min.css><meta property="og:title" content="Creating Novel Art Styles With AI"><meta property="og:description" content="The goal of this project is to enable the creation of new, appealing art-styles with AI. The method used to attempt this is training Stable Diffusion LoRa models on specific art styles, then iteratively merging these into new models and comparing their differences mathematically. The training is achieved by preparing a standard set of images, then using ControlNet to &lsquo;convert&rsquo; them into an artist&rsquo;s style.
The project currently consists of a method of training styled LoRa&rsquo;s, and an interactive program that combines and analyses them."><meta property="og:type" content="article"><meta property="og:url" content="//bemuseed.github.io/posts/novelartstyleswithai/novelartstyleswithai.html"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-02T01:08:11+00:00"><meta property="article:modified_time" content="2023-03-02T01:08:11+00:00"><meta property="og:site_name" content="Caleb Mullan's Site"><meta itemprop=name content="Creating Novel Art Styles With AI"><meta itemprop=description content="The goal of this project is to enable the creation of new, appealing art-styles with AI. The method used to attempt this is training Stable Diffusion LoRa models on specific art styles, then iteratively merging these into new models and comparing their differences mathematically. The training is achieved by preparing a standard set of images, then using ControlNet to &lsquo;convert&rsquo; them into an artist&rsquo;s style.
The project currently consists of a method of training styled LoRa&rsquo;s, and an interactive program that combines and analyses them."><meta itemprop=datePublished content="2023-03-02T01:08:11+00:00"><meta itemprop=dateModified content="2023-03-02T01:08:11+00:00"><meta itemprop=wordCount content="2102"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Creating Novel Art Styles With AI"><meta name=twitter:description content="The goal of this project is to enable the creation of new, appealing art-styles with AI. The method used to attempt this is training Stable Diffusion LoRa models on specific art styles, then iteratively merging these into new models and comparing their differences mathematically. The training is achieved by preparing a standard set of images, then using ControlNet to &lsquo;convert&rsquo; them into an artist&rsquo;s style.
The project currently consists of a method of training styled LoRa&rsquo;s, and an interactive program that combines and analyses them."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=../../ class="f3 fw2 hover-white no-underline white-90 dib"><img src=../../img/avatar.png class="w100 mw5-ns" alt="Caleb Mullan's Site"></a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=../../posts.html title="Posts page">Posts</a></li></ul><div class=ananke-socials><a href=https://github.com/Bemuseed target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Creating Novel Art Styles With AI</h1><p class=tracked>By <strong>Caleb Mullan</strong></p><time class="f6 mv4 dib tracked" datetime=2023-03-02T01:08:11Z>March 2, 2023</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>The goal of this project is to enable the creation of new, appealing art-styles with AI. The method used to attempt this is training Stable Diffusion LoRa models on specific art styles, then iteratively merging these into new models and comparing their differences mathematically. The training is achieved by preparing a standard set of images, then using ControlNet to &lsquo;convert&rsquo; them into an artist&rsquo;s style.</p><p>The project currently consists of a method of training styled LoRa&rsquo;s, and an interactive program that combines and analyses them.</p><h1 id=groundwork-knowledge>Groundwork knowledge</h1><p>Here are some of the core pieces this project builds from if you&rsquo;re not already aware of them:</p><h2 id=stable-diffusion>Stable Diffusion</h2><p>Stable Diffusion is an open-source deep learning model that can generate images from text and other images. A powerful tool called Automatic111 provides a web interface that makes using it much easier.</p><h2 id=lora>LoRa</h2><p><a href=https://arxiv.org/abs/2106.09685>LoRa</a> is a technique for &lsquo;fine-tuning&rsquo; a model such as Stable Diffusion, such that it&rsquo;s output becomes good at outputting certain things, or in our case, outputting in certain style.</p><p>As opposed to techniques like DreamBooth, which fine-tune the entire model, LoRa takes the form of a separate, significantly smaller model that is then applied to SD when images are generated. Using these lets us train, merge and compare significantly faster.</p><p>Automatic1111 has in-built support for LoRa&rsquo;s, and even lets multiple be used at once at different weightings.</p><h2 id=controlnet>ControlNet</h2><p><a href=https://arxiv.org/abs/2302.05543>ControlNet</a> is &rsquo;neural network structure&rsquo; that manages models like Stable Diffusion with additional conditions. In practice, it lets us force SD to output images that replicate the main shapes of other images.</p><p>This means if we want a image to be re-done in a certain style, we pass the original to ControlNet (the &lsquo;canny&rsquo;, edge-detection version) with a prompt for the style, and it creates an images with the same shapes and main lines, but re-stylized elements like color and shading. In theory this will aid us in &lsquo;isolating&rsquo; the styles we want by forcing them to be applied in constrained ways.</p><h1 id=installing-the-necessary-software>Installing the necessary software</h1><p>Automatic1111 can be found <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui>here</a>. Unzip or clone it anywhere you&rsquo;d like. Be aware you&rsquo;ll need a reasonably good graphics card for it to run.</p><p>Assuming you&rsquo;re on Windows, you&rsquo;ll need to edit the <code>webui-user</code> batch file. Next to <code>set PYTHON=</code> copy the full path to your Python installation (this should be at version 3.10, if not you can find it (here)[https://www.python.org/ftp/python/3.10.10/python-3.10.10-amd64.exe]). Next to <code>set COMMANDLINE_ARGS=</code> add <code>--xformers</code> - this will reduce memory usage.</p><p>Running this batch file should bring up a terminal that will give a localhost link to the web application. Once there, go to the Extensions tab, then to Available, click the load button, then search for and install the ControlNet extension. Restart, and it should now be available as a subheading on the image generation screens.</p><p>Finally, go <a href=https://civitai.com/models/9251/controlnet-pre-trained-models>here</a> to download the ControlNet models. Take the &lsquo;canny&rsquo; one, and move it to &rsquo;extensions\sd-webui-controlnet\models&rsquo;.</p><h1 id=lora-training>LoRa training</h1><h2 id=choosing-styles>Choosing styles</h2><p>Any style could be used for this process, but to make things easier we&rsquo;ve opted for artists who Stable Diffusion has been trained on and &ldquo;knows&rdquo;. A list of these can be found archived <a href=https://web.archive.org/web/20230311054225/https://github.com/kaikalii/stable-diffusion-artists>here</a>. Choose styles trained on characters and landscapes, and try to found ones that are distinct to the eye. I opted for Vincent Di Fate, Alan Lee, Michael Garmash, Donato Giancola, Ed Mell and David Mack.</p><h2 id=standard-image-set>Standard image set</h2><p>Styles can be trained on around 20 images or more - the ones I&rsquo;ve used are <a href="https://drive.google.com/drive/folders/1qjo_yja6L_20WbNeUybiqHB34qveuS7X?usp=sharing">here</a>, although this set did change between models as I refined it. To try to get a realistic depiction of the style, the images should be varied in composition, but not too noisy/highly detailed or ControlNet will struggle to distinguish lines. They also have to be at 512x512 resolution. <a href=birme.net>birme.net</a> is useful for cropping in batches.</p><h2 id=generating-stylized-images>Generating stylized images</h2><p>Go to text2img (img2img also works but influences colours to be close to the original - colour choice is an aspect of style so we leave this to the model itself). Enable ControlNet and set it to canny and the relevant model.</p><p>In the prompt box, write &ldquo;[artist name here] art, [basic description of image subject]&rdquo;. For example, &ldquo;Alan Lee art, boy walking on beach&rdquo;. Set the sampler to DDIM at 20 steps(there&rsquo;s a lot of conflicting advice on which to choose, this works best for stylized images in my experience. DPM2 a is also a good choice but doesn&rsquo;t produce anything good until around 50 steps).</p><p>If the image doesn&rsquo;t follow the shapes of the original, check the second output. This shows the lines ControlNet-canny detected. If they&rsquo;re too messy, increase the low and high threshold - too low, decrease them.</p><p>To increase efficiency, you can use the X/Y plot tool in the Scripts section to generate every stylized version of the current image at once. Select Prompt S/R under X type, and writer the names of all the artists you&rsquo;re using in the X values box, comma-separated and starting with one whose name is currently in the prompt. Select &ldquo;Include Sub Images&rdquo; and then generate. It&rsquo;ll do all of them at once, and you can find them in &ldquo;outputs\text2img-images\[date]&rdquo;.</p><h2 id=training>Training</h2><p>Kohya has a useful <a href=https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb>Google Colab notebook</a> for training LoRas. Most of it is self-explanatory if you just go from top-to-bottom. Do note:</p><ul><li>Select StableDiffusion-v1-5 as your model</li><li>After setting the training data directory, open the file manager and add the images to it</li><li>Use the BLIP captioner</li><li>Make sure to put the right path for the pretrained model under Model Config, as well as outputting to Google Drive for convenience</li><li>Set the caption extension to .caption under Dataset Config as this is what BLIP will have created</li><li>Set the epochs to 20 under Training Config, and the save type to .ckpt</li></ul><p>Then run Start Training, it&rsquo;ll take the guts of an hour or so.</p><h2 id=picking-the-best-one>Picking the best one</h2><p>20 .ckpt files will now be in your Drive under &ldquo;LoRa/output&rdquo;. Move these to &ldquo;models\Lora&rdquo; in your Stable Diffusion folder. Pick a suitable prompt, then use the X/Y plot like before, with the X values as the lora names, and the Y value as the strengths (a decimal number between 0 and 1, only a few are needed for comparison&rsquo;s sake). After that&rsquo;s generated you can judge for yourself which model is best - in my experience the 17th usually looks closest to the artist.</p><h1 id=merging-models>Merging models</h1><p>Originally I thought you needed the techniques from the next section, flattening, to do this, but it&rsquo;s actually very straightforward. Load the models like so:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>import</span> torch
</span></span><span style=display:flex><span>model_a <span style=color:#fe8019>=</span> torch<span style=color:#fe8019>.</span>load(<span style=color:#b8bb26>&#34;path_to_a.ckpt&#34;</span>, map_location<span style=color:#fe8019>=</span><span style=color:#b8bb26>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>model_b <span style=color:#fe8019>=</span> torch<span style=color:#fe8019>.</span>load(<span style=color:#b8bb26>&#34;path_to_b.ckpt&#34;</span>, map_location<span style=color:#fe8019>=</span><span style=color:#b8bb26>&#34;cpu&#34;</span>)
</span></span></code></pre></div><p>These are both dictionaries full of the model&rsquo;s data. Pick a ratio you want, like 0.5 or 0.3. Then do this:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_c <span style=color:#fe8019>=</span> <span style=color:#fabd2f>dict</span>()
</span></span><span style=display:flex><span><span style=color:#fe8019>for</span> k <span style=color:#fe8019>in</span> model_a<span style=color:#fe8019>.</span>keys():
</span></span><span style=display:flex><span>    model_c[k] <span style=color:#fe8019>=</span> (ratio <span style=color:#fe8019>*</span> model_a[k]) <span style=color:#fe8019>+</span> ((<span style=color:#d3869b>1</span> <span style=color:#fe8019>-</span> ratio) <span style=color:#fe8019>*</span> model_b[k])
</span></span><span style=display:flex><span><span style=color:#fe8019>return</span> model_c
</span></span></code></pre></div><p>PyTorch Tensor objects (the values in the model dictionaries) support arithmetic with normal operators like + and *, which is why this looks so simple.</p><h1 id=converting-a-model-to-a-flat-array-of-numbers>Converting a model to a flat array of numbers</h1><p>The code for doing this can be found at <a href=https://github.com/Bemuseed/Blog-Git-Repo>this GitHub repo</a>. SD models are, in essence, just multi-dimensional arrays of numbers. To mathematically compare them, we have to be able to access them as a single 1D array of these values, &lsquo;flattening&rsquo; their structure.</p><h2 id=how-its-done>How It&rsquo;s Done</h2><p>Run python from the folder containing the python files. Then:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> <span style=color:#fe8019>import</span> flat<span style=color:#fe8019>,</span> tensor_manager
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> tensors, dict_template <span style=color:#fe8019>=</span> tensor_manager<span style=color:#fe8019>.</span>get_tensors_from_file(<span style=color:#b8bb26>&#34;my_model.ckpt&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> flattened, s, o, l <span style=color:#fe8019>=</span> flat<span style=color:#fe8019>.</span>model_flatten(tensors)
</span></span></code></pre></div><p>This should give you a single numpy array of numbers (of length 18874632 for our LoRa models). A corresponding function, <code>flat.model_unflatten</code>, can restore them to their original state given those s, o, and l values, but for our purposes this isn&rsquo;t needed.</p><h2 id=how-it-works>How It Works</h2><h3 id=tensor_managerpy>tensor_manager.py</h3><p>Every .ckpt file is simply a <a href=https://docs.python.org/3/library/pickle.html>pickled</a> PyTorch model, which can be loaded back into an object using <a href=https://pytorch.org/docs/stable/generated/torch.load.html#torch.load>torch.load</a>. These objects are just dictionaries, whose values are the model Tensor objects.</p><p>The <code>get_tensors</code> method iterates through those values and puts them in a list, and also sets them to 0 to create a &rsquo;template&rsquo; that can be used to repackage the tensors back into the right format, using <code>restore_tensor_dict</code> (which simply reverses the process).</p><h3 id=flatpy>flat.py</h3><p>First, the <code>flatten</code> function in flat.py.</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>flatten</span>(matrix):
</span></span><span style=display:flex><span>    shapes <span style=color:#fe8019>=</span> [a<span style=color:#fe8019>.</span>shape <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix]
</span></span><span style=display:flex><span>    offsets <span style=color:#fe8019>=</span> [a<span style=color:#fe8019>.</span>size <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix]
</span></span><span style=display:flex><span>    offsets <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>cumsum([<span style=color:#d3869b>0</span>] <span style=color:#fe8019>+</span> offsets)
</span></span><span style=display:flex><span>    result <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>concatenate([a<span style=color:#fe8019>.</span>flat <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix])
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> result, shapes, offsets
</span></span></code></pre></div><ol><li>The function first creates a list of shapes, where each shape corresponds to the shape of the corresponding numpy array in the input list. This is done using a list comprehension and the &ldquo;shape&rdquo; attribute of numpy arrays.</li><li>The function then creates a list of offsets, where each offset corresponds to the size (i.e., the number of elements) of the corresponding numpy array in the input list. This is done using a list comprehension and the &ldquo;size&rdquo; attribute of numpy arrays.</li><li>The offsets list is then converted to a cumulative sum using numpy.cumsum(), giving the starting index of each numpy array in the flattened output.</li><li>The function then flattens each numpy array in the input list using the &ldquo;flat&rdquo; attribute of numpy arrays and concatenates them together using numpy.concatenate().</li><li>Finally, the function returns the flattened output, the list of shapes, and the list of offsets.</li></ol><p>The shapes and offsets we get from this are then put to use when we <code>unflatten</code>:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>unflatten</span>(flattened, shapes, offsets):
</span></span><span style=display:flex><span>    restored <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>array([numpy<span style=color:#fe8019>.</span>reshape(flattened[offsets[i]:offsets[i <span style=color:#fe8019>+</span> <span style=color:#d3869b>1</span>]], shape) <span style=color:#fe8019>for</span> i, shape <span style=color:#fe8019>in</span> <span style=color:#fabd2f>enumerate</span>(shapes)])
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> restored
</span></span></code></pre></div><p>This part is better understood if we expand it into a longer for-loop:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>restored <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>array(dtype<span style=color:#fe8019>=</span>numpy<span style=color:#fe8019>.</span>float16)
</span></span><span style=display:flex><span><span style=color:#fe8019>for</span> i, shape <span style=color:#fe8019>in</span> <span style=color:#fabd2f>enumerate</span>(shapes):
</span></span><span style=display:flex><span>	slice_start <span style=color:#fe8019>=</span> offsets[i]
</span></span><span style=display:flex><span>	slice_end <span style=color:#fe8019>=</span> offsets[i <span style=color:#fe8019>+</span> <span style=color:#d3869b>1</span>]
</span></span><span style=display:flex><span>	<span style=color:#fabd2f>slice</span> <span style=color:#fe8019>=</span> flattened[slice_start:slice_end]
</span></span><span style=display:flex><span>	restored_slice <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>reshape(<span style=color:#fabd2f>slice</span>, shape)
</span></span><span style=display:flex><span>	restored <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>append(restored, restored_slice)
</span></span></code></pre></div><p><code>list_flatten</code> simply iterates <code>flatten</code> over a list of arrays (which is what we are given when we run <code>get_tensors_from_file</code>), saving an additional list called <code>limits</code> that marks where in the flattened array the individual arrays start and end. These limits are used to mark each <code>section</code> used in <code>list_unflatten</code>, which also iterates its non-list companion function.</p><h1 id=comparison>Comparison</h1><p>The comparison code in <code>model_compare</code> looks like this:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>compare</span>(model_a_weights, model_b_weights):
</span></span><span style=display:flex><span>    diff <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>abs(model_a_weights <span style=color:#fe8019>-</span> model_b_weights)
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> numpy<span style=color:#fe8019>.</span>average(diff)
</span></span></code></pre></div><p>numpy&rsquo;s functions make this very simple. An array of the difference between every value in the two model&rsquo;s weights is made, then averaged to a single value.</p><h1 id=the-interactive-application>The interactive application</h1><p>This is a fairly straightforward command-line tool. The general working is this:</p><ul><li>A &lsquo;pool&rsquo; of models whose data is currently loaded</li><li>Functions accessible from the menu for<ul><li>Loading from disk</li><li>Saving to disk</li><li>Deleting models from pool</li><li>Comparing models in the pool, then ranking and choosing which to keep</li><li>Merging every unique combination of models in the pool, then comparing, ranking and choosing what to keep</li><li>Iteratively running the above process</li></ul></li></ul><p>I&rsquo;ll explain the last one more closely. Here&rsquo;s it&rsquo;s corresponding code:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>correct <span style=color:#fe8019>=</span> <span style=color:#fe8019>False</span>
</span></span><span style=display:flex><span>    <span style=color:#fe8019>while</span> <span style=color:#fe8019>not</span> correct:
</span></span><span style=display:flex><span>        <span style=color:#fabd2f>print</span>()
</span></span><span style=display:flex><span>        iterations <span style=color:#fe8019>=</span> <span style=color:#fabd2f>max</span>(<span style=color:#d3869b>1</span>, <span style=color:#fabd2f>int</span>(<span style=color:#fabd2f>input</span>(<span style=color:#b8bb26>&#34;Number of iterations: &#34;</span>)))
</span></span><span style=display:flex><span>        ratios <span style=color:#fe8019>=</span> <span style=color:#fabd2f>min</span>(<span style=color:#d3869b>9</span>, <span style=color:#fabd2f>int</span>(<span style=color:#fabd2f>input</span>(<span style=color:#b8bb26>&#34;Number of merges per model pair (1 means a 0.5 merge, 2 means a 0.33 and 0.66, etc.): &#34;</span>)))
</span></span><span style=display:flex><span>        to_keep <span style=color:#fe8019>=</span> <span style=color:#fabd2f>min</span>(<span style=color:#d3869b>1</span>, <span style=color:#fabd2f>int</span>(<span style=color:#fabd2f>input</span>(<span style=color:#b8bb26>&#34;Number of models to keep after each iteration: &#34;</span>)))
</span></span><span style=display:flex><span>        <span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>&#34;Iterations:&#34;</span>, <span style=color:#fabd2f>str</span>(iterations), <span style=color:#b8bb26>&#34;, Ratios: &#34;</span>, <span style=color:#fabd2f>str</span>(ratios), <span style=color:#b8bb26>&#34;, Models to keep: &#34;</span>, <span style=color:#fabd2f>str</span>(to_keep))
</span></span><span style=display:flex><span>        inp <span style=color:#fe8019>=</span> <span style=color:#fabd2f>input</span>(<span style=color:#b8bb26>&#34;Is this correct [y/n]: &#34;</span>)<span style=color:#fe8019>.</span>lower()
</span></span><span style=display:flex><span>        <span style=color:#fe8019>if</span> inp <span style=color:#fe8019>==</span> <span style=color:#b8bb26>&#34;y&#34;</span> <span style=color:#fe8019>or</span> inp <span style=color:#fe8019>==</span> <span style=color:#b8bb26>&#34;yes&#34;</span>:
</span></span><span style=display:flex><span>            correct <span style=color:#fe8019>=</span> <span style=color:#fe8019>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fe8019>for</span> i <span style=color:#fe8019>in</span> <span style=color:#fabd2f>range</span>(iterations):
</span></span><span style=display:flex><span>        <span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>&#34;Iteration &#34;</span> <span style=color:#fe8019>+</span> <span style=color:#fabd2f>str</span>(i <span style=color:#fe8019>+</span> <span style=color:#d3869b>1</span>))
</span></span><span style=display:flex><span>        merged_models, merged_model_names, flattened_merged_models <span style=color:#fe8019>=</span> self<span style=color:#fe8019>.</span>merge_all_and_compare(ratios)
</span></span><span style=display:flex><span>        <span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>&#34;Adding first &#34;</span> <span style=color:#fe8019>+</span> <span style=color:#fabd2f>str</span>(to_keep) <span style=color:#fe8019>+</span> <span style=color:#b8bb26>&#34; to pool... &#34;</span>, end<span style=color:#fe8019>=</span><span style=color:#b8bb26>&#34;&#34;</span>)
</span></span><span style=display:flex><span>        self<span style=color:#fe8019>.</span>models<span style=color:#fe8019>.</span>extend(merged_models[:to_keep])
</span></span><span style=display:flex><span>        self<span style=color:#fe8019>.</span>model_names<span style=color:#fe8019>.</span>extend(merged_model_names[:to_keep])
</span></span><span style=display:flex><span>        self<span style=color:#fe8019>.</span>flattened_models<span style=color:#fe8019>.</span>extend(flattened_merged_models[:to_keep])
</span></span><span style=display:flex><span>        <span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>&#34;done.&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>&#34;Iterations complete.&#34;</span>)
</span></span></code></pre></div><p>At the top, the user choices for number of iterations, number of ratios (thinking of the possible combinations between two models as a straight line, these are the evenly spaced points along it we are looking at), and the number of models to add to the pool each-time (going down from the most different to least).</p><p>Then we loop for the number of iterations, and within it, get the merged models and their data from the function <code>merge_all_and_compare</code>, which is also used by the merging menu item in a much simpler way. These are already in order from most different to least, so we can add their data to the pool (which has three lists for the different data types).</p><h1 id=next-steps-for-the-project>Next steps for the project</h1><p>Here are some possible next-steps for the project:</p><ul><li>General improvements to application, particularly input checks (such that a mistype can never cause a crash).</li><li>Direct comparison between merged LoRa models and the Dreambooth equivalents (given the same training data etc.).</li><li>Using human evaluation via surveys to confirm if mathematical difference is a good metric for stylistic difference.</li></ul><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=//bemuseed.github.io/>&copy; Caleb Mullan's Site 2023</a><div><div class=ananke-socials><a href=https://github.com/Bemuseed target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>