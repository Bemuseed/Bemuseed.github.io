<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>A guide to flattening and unflattening Pytorch models | Caleb Mullan's Site</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Introduction A Pytorch model, like any neural net, is essentially a series of floating point numbers. These are contained within tensors, a form of array, and most models exist as a list of tensors of varying shapes, sizes, and dimensions. This makes mathematical operations on these numbers more difficult, like calculating an average model or performing mutations using techniques like Principal Component Analysis. Even just for comparing models side-by-side, &lsquo;flattening&rsquo; them is useful."><meta name=generator content="Hugo 0.111.1"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=../../ananke/css/main.min.css><meta property="og:title" content="A guide to flattening and unflattening Pytorch models"><meta property="og:description" content="Introduction A Pytorch model, like any neural net, is essentially a series of floating point numbers. These are contained within tensors, a form of array, and most models exist as a list of tensors of varying shapes, sizes, and dimensions. This makes mathematical operations on these numbers more difficult, like calculating an average model or performing mutations using techniques like Principal Component Analysis. Even just for comparing models side-by-side, &lsquo;flattening&rsquo; them is useful."><meta property="og:type" content="article"><meta property="og:url" content="//bemuseed.github.io/posts/flattening-unflattening-stable-diffusion-models/flattening-unflattening-stable-diffusion-models.html"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-03T10:42:09+00:00"><meta property="article:modified_time" content="2023-02-03T10:42:09+00:00"><meta property="og:site_name" content="Caleb Mullan's Site"><meta itemprop=name content="A guide to flattening and unflattening Pytorch models"><meta itemprop=description content="Introduction A Pytorch model, like any neural net, is essentially a series of floating point numbers. These are contained within tensors, a form of array, and most models exist as a list of tensors of varying shapes, sizes, and dimensions. This makes mathematical operations on these numbers more difficult, like calculating an average model or performing mutations using techniques like Principal Component Analysis. Even just for comparing models side-by-side, &lsquo;flattening&rsquo; them is useful."><meta itemprop=datePublished content="2023-02-03T10:42:09+00:00"><meta itemprop=dateModified content="2023-02-03T10:42:09+00:00"><meta itemprop=wordCount content="857"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="A guide to flattening and unflattening Pytorch models"><meta name=twitter:description content="Introduction A Pytorch model, like any neural net, is essentially a series of floating point numbers. These are contained within tensors, a form of array, and most models exist as a list of tensors of varying shapes, sizes, and dimensions. This makes mathematical operations on these numbers more difficult, like calculating an average model or performing mutations using techniques like Principal Component Analysis. Even just for comparing models side-by-side, &lsquo;flattening&rsquo; them is useful."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=../../ class="f3 fw2 hover-white no-underline white-90 dib"><img src=../../img/avatar.png class="w100 mw5-ns" alt="Caleb Mullan's Site"></a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=../../posts.html title="Posts page">Posts</a></li></ul><div class=ananke-socials><a href=https://github.com/Bemuseed target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">A guide to flattening and unflattening Pytorch models</h1><p class=tracked>By <strong>Caleb Mullan</strong></p><time class="f6 mv4 dib tracked" datetime=2023-02-03T10:42:09Z>February 3, 2023</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id=introduction>Introduction</h2><p>A Pytorch model, like any neural net, is essentially a series of floating point numbers. These are contained within tensors, a form of array, and most models exist as a list of tensors of varying shapes, sizes, and dimensions. This makes mathematical operations on these numbers more difficult, like calculating an average model or performing mutations using techniques like Principal Component Analysis. Even just for comparing models side-by-side, &lsquo;flattening&rsquo; them is useful.</p><p><img src=images/neural_net_weights.png alt="A neural net"></p><p>In this guide, you&rsquo;ll be shown how to convert a PyTorch model into a one-dimensional array of floating point numbers, and back again, in the following process:</p><ul><li>Extracting the structure from a .ckpt file.</li><li>&ldquo;Flattening&rdquo; the structure into a 1D Numpy array</li><li>Reformatting this array back to the original format of the model.</li><li>Repackaging this into a .ckpt file once more.</li></ul><h2 id=extraction>Extraction</h2><p>A .ckpt file is just a Python dictionary that&rsquo;s been saved to disk using the &lsquo;pickle&rsquo; module. The Pytorch module has <a href=https://pytorch.org/docs/stable/generated/torch.load.html#torch.load>a load method</a> for loading this into a program. Run the following code in a REPL:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> <span style=color:#fe8019>import</span> torch
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> tensor_dict <span style=color:#fe8019>=</span> torch<span style=color:#fe8019>.</span>load(<span style=color:#b8bb26>&#34;&lt;name-of-model&gt;.ckpt&#34;</span>, map_location<span style=color:#fe8019>=</span><span style=color:#b8bb26>&#34;cpu&#34;</span>)
</span></span></code></pre></div><p>Feel free to look at the dictionary. It consists of a single key-value pair, with the key being the string <code>state_dict</code>, and the value being another dictionary. <em>That</em> dictionary contains 1131 key-value pairs, each value being the Pytorch <code>Tensor</code> objects that contain the numbers we&rsquo;re interested in.</p><p>Now that we know the structure, we can extract those <code>Tensor</code> objects into a list, like so:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> tensors <span style=color:#fe8019>=</span> <span style=color:#fabd2f>list</span>(tensor_dict[<span style=color:#b8bb26>&#39;state_dict&#39;</span>]<span style=color:#fe8019>.</span>values()) 
</span></span></code></pre></div><p>We&rsquo;re also going to need to keep that dictionary structure for later when we want to convert back to a .ckpt file. To simplify things, and free up some memory, we&rsquo;ll set all the values to 0, and keep the keys the same.</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> new_dict <span style=color:#fe8019>=</span> {<span style=color:#b8bb26>&#39;state_dict: dict()} </span>
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> <span style=color:#fe8019>for</span> k <span style=color:#fe8019>in</span> <span style=color:#fabd2f>list</span>(tensor_dict[<span style=color:#b8bb26>&#39;state_dict&#39;</span>]<span style=color:#fe8019>.</span>keys()): 
</span></span><span style=display:flex><span><span style=color:#fe8019>...</span> 	new_dict[<span style=color:#b8bb26>&#39;state_dict&#39;</span>][k] <span style=color:#fe8019>=</span> <span style=color:#d3869b>0</span> 
</span></span></code></pre></div><h2 id=flattening>Flattening</h2><p>Now to actually flatten the tensors. First, we&rsquo;ll convert them all to Numpy arrays, which <code>Pytorch</code> provides an in-built method for.</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> tensors <span style=color:#fe8019>=</span> [t<span style=color:#fe8019>.</span>numpy() <span style=color:#fe8019>for</span> t <span style=color:#fe8019>in</span> tensors]
</span></span></code></pre></div><p>Then, we perform the flattening itself. First we&rsquo;ll make the following function to take a single tensor array and flatten it:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>flatten</span>(matrix):
</span></span><span style=display:flex><span>    shapes <span style=color:#fe8019>=</span> [a<span style=color:#fe8019>.</span>shape <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix]
</span></span><span style=display:flex><span>    offsets <span style=color:#fe8019>=</span> [a<span style=color:#fe8019>.</span>size <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix]
</span></span><span style=display:flex><span>    offsets <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>cumsum([<span style=color:#d3869b>0</span>] <span style=color:#fe8019>+</span> offsets)
</span></span><span style=display:flex><span>    result <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>concatenate([a<span style=color:#fe8019>.</span>flat <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix])
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> result, shapes, offsets
</span></span></code></pre></div><p>While you could type that out in the REPL, I&rsquo;d recommend you put this in it&rsquo;s own .py file. Then you can use it in a REPL with an import or with</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> exec(<span style=color:#fabd2f>open</span>(<span style=color:#b8bb26>&#34;&lt;file&gt;.py).read())</span>
</span></span></code></pre></div><p>Then, we use this function in another function that applies this method to a list of different matrices:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>list_flatten</span>(matrix_list):
</span></span><span style=display:flex><span>    result <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>array([])
</span></span><span style=display:flex><span>    shapes <span style=color:#fe8019>=</span> []
</span></span><span style=display:flex><span>    offsets <span style=color:#fe8019>=</span> []
</span></span><span style=display:flex><span>    limits <span style=color:#fe8019>=</span> [<span style=color:#d3869b>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fe8019>for</span> a <span style=color:#fe8019>in</span> matrix_list:
</span></span><span style=display:flex><span>        r, s, o <span style=color:#fe8019>=</span> flatten(a)
</span></span><span style=display:flex><span>        result <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>append(result, r)
</span></span><span style=display:flex><span>        shapes<span style=color:#fe8019>.</span>append(s)
</span></span><span style=display:flex><span>        offsets<span style=color:#fe8019>.</span>append(o)
</span></span><span style=display:flex><span>        limits<span style=color:#fe8019>.</span>append(<span style=color:#fabd2f>len</span>(result))
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> result, shapes, offsets, limits
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> flattened, shapes, offsets, limits <span style=color:#fe8019>=</span> list_flatten(tensors)
</span></span></code></pre></div><p>Now you have your 1D array! We can now manipulate it as we please. For example, let&rsquo;s see how many floating point numbers are in the whole model:</p><p><img src=images/tensor_size.png alt="The total number of numbers in the model"></p><h2 id=unflattening>Unflattening</h2><p>This part is the more complex of the two. To begin, a twin for the flattening function:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>unflatten</span>(flattened, shapes, offsets):
</span></span><span style=display:flex><span>    restored <span style=color:#fe8019>=</span> numpy<span style=color:#fe8019>.</span>array([numpy<span style=color:#fe8019>.</span>reshape(flattened[offsets[i]:offsets[i <span style=color:#fe8019>+</span> <span style=color:#d3869b>1</span>]], shape)
</span></span><span style=display:flex><span>            <span style=color:#fe8019>for</span> i, shape <span style=color:#fe8019>in</span> <span style=color:#fabd2f>enumerate</span>(shapes)])
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> restored
</span></span></code></pre></div><h3 id=a-brief-explanation>A Brief Explanation</h3><p>There&rsquo;s a lot going on in that middle line, so here&rsquo;s a quick breakdown:</p><ul><li>That <code>offsets</code> variable we made earlier keeps track of starting and ending indices in the flattened 1D array, and the <code>shapes</code> variable to track their shapes.</li><li>A list comprehension is used to loop over both of these variables, storing the current ones in the <code>i</code> and <code>shape</code> variables within the comprehension.</li><li>In this part:</li></ul><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>flattened[offsets[i]:offsets[i <span style=color:#fe8019>+</span> <span style=color:#d3869b>1</span>]]
</span></span></code></pre></div><ul><li>&mldr; the slice of the flattened array that corresponds to the current original array is determined. <code>offsets[i]</code> is the starting index, and <code>offsets[i+1]</code> the ending index.
-<code>numpy.reshape is used with this and the </code>shape``` variable as its arguments, to reshape the slice of the flat array into its original shape.</li></ul><h3 id=moving-on>Moving On</h3><p>And now a corresponding function for <code>list_flatten</code>:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>def</span> <span style=color:#fabd2f>list_unflatten</span>(flattened, shapes, offsets, limits):
</span></span><span style=display:flex><span>    unflattened <span style=color:#fe8019>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#fe8019>for</span> i <span style=color:#fe8019>in</span> <span style=color:#fabd2f>range</span>(<span style=color:#fabd2f>len</span>(limits) <span style=color:#fe8019>-</span> <span style=color:#d3869b>1</span>):
</span></span><span style=display:flex><span>        section <span style=color:#fe8019>=</span> flattened[limits[i]:limits[i<span style=color:#fe8019>+</span><span style=color:#d3869b>1</span>]]
</span></span><span style=display:flex><span>        unflattened<span style=color:#fe8019>.</span>append(unflatten(section, shapes[i], offsets[i]))
</span></span><span style=display:flex><span>    <span style=color:#fe8019>return</span> unflattened
</span></span></code></pre></div><p>Which you can then use with the output of the flattening functions:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> restored_tensor_list <span style=color:#fe8019>=</span> list_unflatten(flattened, shapes, offsets, limits)
</span></span></code></pre></div><p>(A word of warning - running this part will eat up your RAM. Make sure to free some up before running it. Linux users without a lot of memory who can increase their swap partition size should do so.)</p><h2 id=repackaging>Repackaging</h2><p>And finally, to take our modified tensor list and place it in a new .ckpt file, we first return it to its original dictionary format. First, we convert the arrays back into <code>Tensor</code> objects:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> tensors <span style=color:#fe8019>=</span> [torch<span style=color:#fe8019>.</span>from_numpy(nd) <span style=color:#fe8019>for</span> nd <span style=color:#fe8019>in</span> tensors]
</span></span></code></pre></div><p>Then we use that <code>dict_template</code> dictionary we prepared earlier, and populate it with our new tensor data:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> t_counter <span style=color:#fe8019>=</span> <span style=color:#d3869b>0</span>
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> new_dict <span style=color:#fe8019>=</span> {<span style=color:#b8bb26>&#39;state_dict&#39;</span>: <span style=color:#fabd2f>dict</span>()}
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> <span style=color:#fe8019>for</span> k <span style=color:#fe8019>in</span> <span style=color:#fabd2f>list</span>(dict_template[<span style=color:#b8bb26>&#39;state_dict&#39;</span>]<span style=color:#fe8019>.</span>keys()):
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> 	new_dict[<span style=color:#b8bb26>&#39;state_dict&#39;</span>][k] <span style=color:#fe8019>=</span> tensors[t_counter]
</span></span><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span>     t_counter <span style=color:#fe8019>+=</span> <span style=color:#d3869b>1</span>
</span></span></code></pre></div><p>And finally, we use the Pytorch save method to produce our new .ckpt file.</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fe8019>&gt;&gt;&gt;</span> torch<span style=color:#fe8019>.</span>save(new_dict, <span style=color:#b8bb26>&#34;my_model&#34;</span><span style=color:#fe8019>.</span>ckpt)
</span></span></code></pre></div><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=//bemuseed.github.io/>&copy; Caleb Mullan's Site 2023</a><div><div class=ananke-socials><a href=https://github.com/Bemuseed target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>